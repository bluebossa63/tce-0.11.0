apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: vault
  name: hashicorp-vault
  namespace: vault
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: jenkins-tls-cert
  namespace: vault
spec:
  commonName: vault.ne.local
  dnsNames:
  - vault.ne.local
  duration: 87600h0m0s
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: niceneasy-ca
  renewBefore: 360h0m0s
  secretName: jenkins-tls
  usages:
  - server auth
  - client auth
---
apiVersion: projectcontour.io/v1
kind: HTTPProxy
metadata:
  labels:
    app: jenkins
  name: jenkins-httpproxy
  namespace: vault
spec:
  routes:
  - conditions:
    - prefix: /
    services:
    - name: tph-jenkins
      port: 80
  virtualhost:
    fqdn: vault.ne.local
    tls:
      secretName: jenkins-tls
---
apiVersion: v1
kind: Pod
metadata:
  name: hashicorp-vault-server-test
  namespace: vault
  annotations:
    helm.sh/hook: test
spec:
  containers:
  - name: hashicorp-vault-server-test
    image: hashicorp/vault:1.9.2
    imagePullPolicy: IfNotPresent
    env:
    - name: VAULT_ADDR
      value: http://hashicorp-vault.default.svc:8200
    command:
    - /bin/sh
    - -c
    - |
      echo "Checking for sealed info in 'vault status' output"
      ATTEMPTS=10
      n=0
      until [ "$n" -ge $ATTEMPTS ]
      do
        echo "Attempt" $n...
        vault status -format yaml | grep -E '^sealed: (true|false)' && break
        n=$((n+1))
        sleep 5
      done
      if [ $n -ge $ATTEMPTS ]; then
        echo "timed out looking for sealed info in 'vault status' output"
        exit 1
      fi

      exit 0
    volumeMounts: null
  volumes: null
  restartPolicy: Never
MANIFEST: null
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hashicorp-vault-csi-provider
  namespace: vault
  labels:
    app.kubernetes.io/name: vault-csi-provider
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hashicorp-vault
  namespace: vault
  labels:
    helm.sh/chart: vault-0.19.0
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: hashicorp-vault-csi-provider-clusterrole
  labels:
    app.kubernetes.io/name: vault-csi-provider
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
  namespace: vault
rules:
- apiGroups:
  - ""
  resources:
  - serviceaccounts/token
  verbs:
  - create
---
apiVersion: v1
kind: Service
metadata:
  name: hashicorp-vault-internal
  namespace: vault
  labels:
    helm.sh/chart: vault-0.19.0
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
  annotations: null
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: http
    port: 8200
    targetPort: 8200
  - name: https-internal
    port: 8201
    targetPort: 8201
  selector:
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: hashicorp-vault
    component: server
---
apiVersion: v1
kind: Service
metadata:
  name: hashicorp-vault
  namespace: vault
  labels:
    helm.sh/chart: vault-0.19.0
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
  annotations: null
spec:
  publishNotReadyAddresses: true
  ports:
  - name: http
    port: 8200
    targetPort: 8200
  - name: https-internal
    port: 8201
    targetPort: 8201
  selector:
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: hashicorp-vault
    component: server
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hashicorp-vault-csi-provider
  namespace: vault
  labels:
    app.kubernetes.io/name: vault-csi-provider
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: vault-csi-provider
      app.kubernetes.io/instance: hashicorp-vault
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vault-csi-provider
        app.kubernetes.io/instance: hashicorp-vault
    spec:
      serviceAccountName: hashicorp-vault-csi-provider
      containers:
      - name: vault-csi-provider
        image: hashicorp/vault-csi-provider:0.4.0
        imagePullPolicy: IfNotPresent
        args:
        - --endpoint=/provider/vault.sock
        - --debug=false
        volumeMounts:
        - name: providervol
          mountPath: /provider
        - name: mountpoint-dir
          mountPath: /var/lib/kubelet/pods
          mountPropagation: HostToContainer
        livenessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 3
      volumes:
      - name: providervol
        hostPath:
          path: /etc/kubernetes/secrets-store-csi-providers
      - name: mountpoint-dir
        hostPath:
          path: /var/lib/kubelet/pods
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hashicorp-vault
  namespace: vault
  labels:
    app.kubernetes.io/name: vault
    app.kubernetes.io/instance: hashicorp-vault
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: hashicorp-vault-internal
  podManagementPolicy: Parallel
  replicas: 1
  updateStrategy:
    type: OnDelete
  selector:
    matchLabels:
      app.kubernetes.io/name: vault
      app.kubernetes.io/instance: hashicorp-vault
      component: server
  template:
    metadata:
      labels:
        helm.sh/chart: vault-0.19.0
        app.kubernetes.io/name: vault
        app.kubernetes.io/instance: hashicorp-vault
        component: server
    spec:
      terminationGracePeriodSeconds: 10
      serviceAccountName: hashicorp-vault
      securityContext:
        runAsNonRoot: true
        runAsGroup: 1000
        runAsUser: 100
        fsGroup: 1000
      volumes:
      - name: home
        emptyDir: {}
      containers:
      - name: vault
        image: hashicorp/vault:1.9.2
        imagePullPolicy: IfNotPresent
        command:
        - /bin/sh
        - -ec
        args:
        - "/usr/local/bin/docker-entrypoint.sh vault server -dev \n"
        securityContext:
          allowPrivilegeEscalation: false
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VAULT_K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: VAULT_K8S_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: VAULT_ADDR
          value: http://127.0.0.1:8200
        - name: VAULT_API_ADDR
          value: http://$(POD_IP):8200
        - name: SKIP_CHOWN
          value: "true"
        - name: SKIP_SETCAP
          value: "true"
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: VAULT_CLUSTER_ADDR
          value: https://$(HOSTNAME).hashicorp-vault-internal:8201
        - name: HOME
          value: /home/vault
        - name: VAULT_DEV_ROOT_TOKEN_ID
          value: root
        - name: VAULT_DEV_LISTEN_ADDRESS
          value: '[::]:8200'
        volumeMounts:
        - name: home
          mountPath: /home/vault
        ports:
        - containerPort: 8200
          name: http
        - containerPort: 8201
          name: https-internal
        - containerPort: 8202
          name: http-rep
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -ec
            - vault status -tls-skip-verify
          failureThreshold: 2
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 3
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - sleep 5 && kill -SIGTERM $(pidof vault)
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-secrets-store-csi-driver-upgrade-crds
  namespace: vault
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "1"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-secrets-store-csi-driver-keep-crds
  namespace: vault
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "2"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: csi-secrets-store-csi-driver-upgrade-crds
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "1"
  namespace: vault
rules:
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - create
  - update
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: csi-secrets-store-csi-driver-keep-crds
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "2"
  namespace: vault
rules:
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - patch
---
apiVersion: batch/v1
kind: Job
metadata:
  name: secrets-store-csi-driver-upgrade-crds
  namespace: vault
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "1"
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
spec:
  backoffLimit: 0
  template:
    metadata:
      name: csi-secrets-store-csi-driver-upgrade-crds
    spec:
      serviceAccountName: csi-secrets-store-csi-driver-upgrade-crds
      restartPolicy: Never
      containers:
      - name: crds-upgrade
        image: k8s.gcr.io/csi-secrets-store/driver-crds:v1.1.2
        args:
        - apply
        - -f
        - crds/
        imagePullPolicy: IfNotPresent
      nodeSelector:
        kubernetes.io/os: linux
---
apiVersion: batch/v1
kind: Job
metadata:
  name: secrets-store-csi-driver-keep-crds
  namespace: vault
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-weight: "2"
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
spec:
  backoffLimit: 0
  template:
    metadata:
      name: csi-secrets-store-csi-driver-keep-crds
    spec:
      serviceAccountName: csi-secrets-store-csi-driver-keep-crds
      restartPolicy: Never
      containers:
      - name: crds-keep
        image: k8s.gcr.io/csi-secrets-store/driver-crds:v1.1.2
        args:
        - patch
        - crd
        - secretproviderclasses.secrets-store.csi.x-k8s.io
        - secretproviderclasspodstatuses.secrets-store.csi.x-k8s.io
        - -p
        - '{"metadata":{"annotations": {"helm.sh/resource-policy": "keep"}}}'
        imagePullPolicy: IfNotPresent
      nodeSelector:
        kubernetes.io/os: linux
MANIFEST: null
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: secrets-store-csi-driver
  namespace: vault
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: secretproviderclasses-admin-role
  namespace: vault
rules:
- apiGroups:
  - secrets-store.csi.x-k8s.io
  resources:
  - secretproviderclasses
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: secretproviderclasses-viewer-role
  namespace: vault
rules:
- apiGroups:
  - secrets-store.csi.x-k8s.io
  resources:
  - secretproviderclasses
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: secretproviderclasses-role
  namespace: vault
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - secrets-store.csi.x-k8s.io
  resources:
  - secretproviderclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - secrets-store.csi.x-k8s.io
  resources:
  - secretproviderclasspodstatuses
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - secrets-store.csi.x-k8s.io
  resources:
  - secretproviderclasspodstatuses/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - storage.k8s.io
  resourceNames:
  - secrets-store.csi.k8s.io
  resources:
  - csidrivers
  verbs:
  - get
  - list
  - watch
---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: csi-secrets-store-csi-driver
  namespace: vault
  labels:
    app.kubernetes.io/instance: csi
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: secrets-store-csi-driver
    app.kubernetes.io/version: 1.1.2
    app: secrets-store-csi-driver
    helm.sh/chart: secrets-store-csi-driver-1.1.2
spec:
  selector:
    matchLabels:
      app: secrets-store-csi-driver
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: secrets-store
      labels:
        app.kubernetes.io/instance: csi
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: secrets-store-csi-driver
        app.kubernetes.io/version: 1.1.2
        app: secrets-store-csi-driver
        helm.sh/chart: secrets-store-csi-driver-1.1.2
    spec:
      serviceAccountName: secrets-store-csi-driver
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: type
                operator: NotIn
                values:
                - virtual-kubelet
      containers:
      - name: node-driver-registrar
        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0
        args:
        - --v=5
        - --csi-address=/csi/csi.sock
        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-secrets-store/csi.sock
        livenessProbe:
          exec:
            command:
            - /csi-node-driver-registrar
            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-secrets-store/csi.sock
            - --mode=kubelet-registration-probe
          initialDelaySeconds: 30
          timeoutSeconds: 15
        env:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: plugin-dir
          mountPath: /csi
        - name: registration-dir
          mountPath: /registration
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 10m
            memory: 20Mi
      - name: secrets-store
        image: k8s.gcr.io/csi-secrets-store/driver:v1.1.2
        args:
        - --endpoint=$(CSI_ENDPOINT)
        - --nodeid=$(KUBE_NODE_NAME)
        - --provider-volume=/etc/kubernetes/secrets-store-csi-providers
        - --additional-provider-volume-paths=/var/run/secrets-store-csi-providers
        - --metrics-addr=:8095
        - --provider-health-check-interval=2m
        - --max-call-recv-msg-size=4194304
        env:
        - name: CSI_ENDPOINT
          value: unix:///csi/csi.sock
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        ports:
        - containerPort: 9808
          name: healthz
          protocol: TCP
        - containerPort: 8095
          name: metrics
          protocol: TCP
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: healthz
          initialDelaySeconds: 30
          timeoutSeconds: 10
          periodSeconds: 15
        volumeMounts:
        - name: plugin-dir
          mountPath: /csi
        - name: mountpoint-dir
          mountPath: /var/lib/kubelet/pods
          mountPropagation: Bidirectional
        - name: providers-dir
          mountPath: /etc/kubernetes/secrets-store-csi-providers
        - name: providers-dir-0
          mountPath: /var/run/secrets-store-csi-providers
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 50m
            memory: 100Mi
      - name: liveness-probe
        image: k8s.gcr.io/sig-storage/livenessprobe:v2.6.0
        imagePullPolicy: IfNotPresent
        args:
        - --csi-address=/csi/csi.sock
        - --probe-timeout=3s
        - --http-endpoint=0.0.0.0:9808
        - -v=2
        volumeMounts:
        - name: plugin-dir
          mountPath: /csi
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 10m
            memory: 20Mi
      volumes:
      - name: mountpoint-dir
        hostPath:
          path: /var/lib/kubelet/pods
          type: DirectoryOrCreate
      - name: registration-dir
        hostPath:
          path: /var/lib/kubelet/plugins_registry/
          type: Directory
      - name: plugin-dir
        hostPath:
          path: /var/lib/kubelet/plugins/csi-secrets-store/
          type: DirectoryOrCreate
      - name: providers-dir
        hostPath:
          path: /etc/kubernetes/secrets-store-csi-providers
          type: DirectoryOrCreate
      - name: providers-dir-0
        hostPath:
          path: /var/run/secrets-store-csi-providers
          type: DirectoryOrCreate
      nodeSelector:
        kubernetes.io/os: linux
---
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: secrets-store.csi.k8s.io
  namespace: vault
spec:
  podInfoOnMount: true
  attachRequired: false
  volumeLifecycleModes:
  - Ephemeral
